## 获取Noisy-Clean数据对
```python
from pathlib import Path
import os

def get_all_wavs(root):
    # 先获取目录下的所有.wav文件
    files = []
    for p in Path(root).iterdir():
        if str(p).endswith(".wav"):
            files.append(str(p))
        for s in p.rglob('*.wav'):
            files.append(str(s))
    return list(set(files))

def get_pair_data(root):
    # 筛选出其中的Noisy数据（数据名包含了fileid信息，可以以此获取到Clean数据）
    datas = []
    for idx, wav_path in enumerate(get_all_wavs(root)):
        id_ = os.path.basename(wav_path).split("_")[-1].split(".")[0]
        if str(os.path.basename(wav_path)).startswith("clean_fileid_"):
            continue
        noisy_path = wav_path
        clean_path = os.path.join(os.path.dirname(wav_path), "../clean", "clean_fileid_%s.wav" % id_)
        datas.append((noisy_path, clean_path))
    return datas
```

## 创建Dataset对象

基本与Pytorch一样，唯一的区别是不用继承某个父类，只要实现了__getitem__和__len__关键函数就行了

```python
class SEDataset:
    def __init__(self, root, audio_len, batch_size):
        self.data = get_pair_data(root)
        self.audio_len = audio_len
        self.batch_size = batch_size
        self.data = np.array_split(np.array(self.data), len(self.data)//batch_size)
    
    def __getitem__(self, index):
        batches = self.data[index]
        return batches

    def __len__(self):
        return len(self.data)
```

这里的示例代码是以Dataset->Sampler的形式组建的，所以在Dataset里提前根据batch_size会将数据进行分组，每次get_item都会直接返回一个batch的数据（Noisy和Clean的路径）

```txt
拓展疑问：
为啥要提前分？像pytorch那样让DataLoader自己读batch不行吗？

解答：
这里涉及到MindSpore的计算模式：图模式。熟悉Tensorflow的同学可以直接跳到第二段。

啥是图模式？我们知道，Pytorch可以动态地调整batch的大小，只要保证每个batch内的tensor维度是一致的就行，比如我batch1可以是(2,55)，batch2可以是(66,18)。但是Tensorflow（老版本的）和MindSpore是不支持这种操作的，它们的每个batch都需要是等长的。通俗地讲，图模式就是在计算初期创建好图（可以认为是一组互连的管道），图内所有节点的Tensor维度都是一来就设置好的，不能在计算的途中改变。

图模型对于大部分的CV任务来讲，影响不会太大，但是语音序列长度基本全是变长的，我们难道必须把2s的语音全部pad到30s去扔给模型训练吗，太浪费资源了。所以我们这里使用了数据分桶的机制来尝试缓解这个问题（只是从完全不适用变得勉强可用了，并没有解决根本问题）。

分桶机制，MindSpore不是需要提前设置好输入的维度嘛，所以我们就提前把语音按长度分桶，如0s~5s、5s~10s、10s~15s 这样去把原本的语音数据集分成很多个小桶，然后我们给每个桶创建一个图，在计算时根据数据的长度去选择相应的桶就行了。但是经过我们测试，8个桶比较稳定，超过12个桶就容易爆内存了。。。所以这是一个勉强能用的策略。

回到代码，我们提前根据长度把数据分好batch，（这里为了方便我举的Speech enhancement例子，语音仿真时都固定了长度，所以上述问题不存在，但是为了让各位能读懂我们后面其他语音模型的代码，这里提前给大家讲一下为啥要这样做）
```

## 为Dataset绑定Sampler操作

```python
def get_dataset(train_root, test_root, batch_size, num_worker, audio_len):
    train_dataset = de.GeneratorDataset(
        source=SEDataset(root=train_root, audio_len=audio_len, batch_size=batch_size),
        column_names=["batches"], # 这里要与Dataset中getitem的输出的名字保持一致
        num_parallel_workers=num_worker,
        shuffle=True
    )
    if test_root is not None:
        test_dataset = de.GeneratorDataset(
            source=SEDataset(root=test_root, audio_len=audio_len, batch_size=batch_size),
            column_names=["batches"],
            num_parallel_workers=num_worker,
            shuffle=False
        )
    
    def padding(x, audio_len):
        if len(x) > audio_len:
            return x[:audio_len]
        elif len(x) < audio_len:
            return np.pad(x, audio_len-len(x))
        else:
            return x
        
    def stft(x):
        x = lib.stft(x, n_fft=512, hop_length=128, win_length=512, window='hann')
        x = np.concatenate([x.real, x.imag], axis=1)
        return x
        
    def sampler(batches):
        # Sampler函数，输入的就是get_item的输出，后续读我们写的HuBERT代码可以看到，多级多卡时数据的划分也是在这里完成的（根据rank_id去分）
        noisy_batch = []
        clean_batch = []
        noisy_cplx_batch = []
        clean_cplx_batch = []
        for noisy_path, clean_path in batches:
            noisy = lib.load(noisy_path, sr=16000)[0]
            clean = lib.load(clean_path, sr=16000)[0]
            noisy = padding(noisy, audio_len)
            clean = padding(clean, audio_len)
            noisy_batch.append(noisy)
            clean_batch.append(clean)
            
        noisy_batch = np.stack(noisy_batch, axis=0)
        clean_batch = np.stack(clean_batch, axis=0)
        # 写到循环外面会稍微快一些，但是你们试了就知道了，华为的CPU很慢，所以在做训练的时候这里非常不推荐on-the-fly地去处理数据（这里stft都会使模型训练非常慢。。。。），推荐提前把特征提取好存下来，后面通过路径去读
        noisy_cplx_batch = stft(noisy_batch) 
        clean_cplx_batch = stft(clean_batch)
        return noisy_batch, clean_batch, noisy_cplx_batch, clean_cplx_batch

    train_dataset = train_dataset.map(
        operations=sampler, 
        input_columns=[
            'batches'
        ], # sampler 函数的输入变量名
        output_columns=[
            'noisy_batch', 'clean_batch', 'noisy_cplx_batch', 'clean_cplx_batch'
        ], # sampler 函数的输出变量名，这些名字必须对应上，不然在建图的时候会报错
        num_parallel_workers=num_worker
    ).project([
        'noisy_batch', 'clean_batch', 'noisy_cplx_batch', 'clean_cplx_batch'
    ])
    if test_root is not None:
        test_dataset = test_dataset.map(
            operations=sampler, 
            input_columns=[
                'batches'
            ],
            output_columns=[
                'noisy_batch', 'clean_batch', 'noisy_cplx_batch', 'clean_cplx_batch'
            ],
            num_parallel_workers=num_worker
        ).project([
            'noisy_batch', 'clean_batch', 'noisy_cplx_batch', 'clean_cplx_batch'
        ])
    else:
        test_dataset = None
    
    return train_dataset, test_dataset
```