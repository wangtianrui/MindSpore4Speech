## NsNet

这里用了一个非常简单的语音增强模型：NsNet，其为2020年DNS-Challenge的baseline系统。

#### 强调一件事儿：Mindspore的图模式是不支持训练时改变数据的走向，举个例子：pytorch可以实现 loss = self.loss1(a, b) if self.step % 2 == 0 else self.loss2(a, b)，但是这种逻辑在MindSpore是不支持的，如果有这种逻辑会爆非常抽象的硬件错误，很难debug出来，建议写的时候就避免这种写法。！！！！！！！！！！！！！！这个非常重要。。。血的教训

```python
class NsNet(nn.Cell):
    def __init__(self):
        super().__init__()
        """
        模型很简单，输入是257维的对数幅度谱，然后经过FC->GRU->FC->FC->FC
        最后通过Sigmoid函数输出Mask
        在模型具体实现时，我也尝试过用卷积去实现STFT和iSTFT
        （之前不是说cpu处理慢嘛，尝试了一下转到GPU，但更慢了，哈哈哈哈，感兴趣的同学可以试试优化一下）
        """
        self.encoder = nn.SequentialCell(
            nn.Dense(257, 400, weight_init="normal", bias_init="zeros"),
            nn.PReLU(),
            nn.LayerNorm((400,))
        )
        self.rnn = nn.GRU(400, 400, 2, has_bias=True, batch_first=True, bidirectional=False)
        self.decoder = nn.SequentialCell(
            nn.Dense(400, 600, weight_init="normal", bias_init="zeros"),
            nn.LayerNorm((600,)),
            nn.PReLU(),
            nn.Dense(600, 600, weight_init="normal", bias_init="zeros"),
            nn.LayerNorm((600,)),
            nn.PReLU(),
            nn.Dense(600, 257, weight_init="normal", bias_init="zeros"),
            nn.Sigmoid(),
        )
        self.mse = nn.MSELoss()
    
    def construct(self, noisy_batch, clean_batch, noisy_cplx_batch, clean_cplx_batch):
        # 等价于Pytorch的forward
        # B, F*2, T
        # 输入是复数谱，所以先计算对数幅度谱
        real, imag = noisy_cplx_batch[:, :self.nfft//2+1, :], noisy_cplx_batch[:, -(self.nfft//2+1):, :]
        magnitude = ((real ** 2 + imag ** 2 + 1e-8) ** 0.5).log().permute(0, 2, 1)
        
        # 经过模型计算出
        ff_result = self.encoder(magnitude)
        rnn_result, _ = self.rnn(ff_result)
        mask = self.decoder(rnn_result).permute(0, 2, 1)

        # mask apply 到 noisy 上，这里相当于只处理了幅度，保留了noisy的相位
        real_result = real * mask
        imag_result = imag * mask
        est_cplx = mindspore.ops.cat([real_result, imag_result], 1)
        
        # loss
        loss = self.mse(est_cplx, clean_cplx_batch)
        return loss

    def infer(self, noisy_cplx_batch):
        # 推理
        real, imag = noisy_cplx_batch[:, :self.nfft//2+1, :], noisy_cplx_batch[:, -(self.nfft//2+1):, :]
        magnitude = ((real ** 2 + imag ** 2 + 1e-8) ** 0.5).log().permute(0, 2, 1)
        
        ff_result = self.encoder(magnitude)
        rnn_result, _ = self.rnn(ff_result)
        mask = self.decoder(rnn_result).permute(0, 2, 1)
        real_result = real * mask
        imag_result = imag * mask
        est_cplx = mindspore.ops.cat([real_result, imag_result], 1)
        return est_cplx
```

## 梯度计算自定义

模型创建好了，虽然可以直接训练了，但是我们经常会有自定义梯度的需求，如GAN模型的训练。这个时候需要手动实现TrainOneStepCell

```python
class OneStep(TrainOneStepCell):
    def __init__(self, model, optimizer):
        super(OneStep, self).__init__(model, optimizer)
        # 输入模型和优化器
        # 感兴趣的同学可以去读一下TrainOneStepCell的源码，基本逻辑是写好了模型训练的梯度更新方式，如果我们训练的模型不是这个类的子类，那么他会自动给你套一个的
        self.network.set_train()
        self.network.set_grad()
    
    def construct(self, *inputs):
        batch_size = inputs[0].shape[0]
        loss = self.network(*inputs) # 模型的前向
        grads = self.grad_no_sens(self.network, self.weights)(*inputs) # 后向
        grads = self.grad_reducer(grads) # 多卡的梯度融合到一起
        loss = F.depend(loss, self.optimizer(grads)) # 绑定优化器，完成参数的更新
        return loss, batch_size
```