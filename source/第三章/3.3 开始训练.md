## 使用Model类开始训练

MindSpore提供了一个Model类，里面封装了所有的训练代码（包括优化器绑定、数据流的走向、损失函数的统计、回调接口的开放等）

```python
# 创建模型和OneStep的对象
model = NsNet(nfft=512, hop_len=60)
logger.info("model inited")
optimizer = Adam(params=model.trainable_params(), learning_rate=0.00005, weight_decay=0.000001)
one_step = OneStep(model, optimizer)
# 数据集
train_dataset, test_dataset = get_dataset(
    train_root=r"/home/ma-user/work/ms_tutorial_speech/data/1H_-5to20",
    test_root=None,
    batch_size=80,
    num_worker=1,
    audio_len=10*16000
)
# 回调器的声明，相当于模型每step训练完都会把回调器里的方法走一遍，学习率的修改、日志的打印、模型的保存都可以写到这里
callback_list = [
    Logger(len(train_dataset), test_dataset, log_interval=10, logger=logger, model=EvalOneStep(model)), 
    ModelCheckpoint(directory="./exp", config=CheckpointConfig(save_checkpoint_steps=1000, keep_checkpoint_max=5)),
]
# 给我们的模型套上Model
model = mindspore.Model(one_step)
logger.info("training start")
# 开始训练
model.fit(800, train_dataset, callbacks=callback_list)
```


## 回调器

官方提供了很多的回调器，如学习率调整、模型保存、日志打印等。这里我们实现一个简单的日志自定义给大家示范一下

```python
class Logger(Callback):
    """
    Callback父类有很多接口函数，如on_train_epoch_begin等，Model训完相应的逻辑后就会执行，如on_train_epoch_begin就是在每个epoch开始的时候会调用一次
    """
    def __init__(self, dataset_size, eval_dataset, log_interval, logger, model):
        super().__init__()
        self.dataset_size = dataset_size
        # Eval也可以写到这里来自定义的
        self.eval_dataset = eval_dataset
        self.model = model
        try:
            self.rank = get_rank() # 这个主要是多级多卡的时候做一个区别
        except (ValueError, RuntimeError):
            self.rank = 0
        self.log_interval = log_interval
        self.logger = logger
        self.step = 0
        self.losses = []
    
    def on_train_epoch_begin(self, run_context):
        self.model.set_train()
    
    def on_train_step_begin(self, run_context):
        self.step_time = time.time() # step计时
    
    def on_train_step_end(self, run_context):
        step_seconds = time.time() - self.step_time # step计时
        cb_params = run_context.original_args() # 模型的输出会返回到这里
        # cb_params.net_outputs为模型的输出
        # 之前我们自定义的OneStep会返回loss和batch，所以[0]是loss
        loss = cb_params.net_outputs[0].asnumpy() 
        self.losses.append(loss) # 记下来
        batchsize = cb_params.net_outputs[1]
        if self.step % 10 == 0: # 10 step打印一次，打印完了losses的缓存清零
            self.logger.info(
                "[Train]Epoch:%d; Step:%d; Time:%.2f; Loss:%s; BS:%d" % (
                    int(self.step/self.dataset_size), 
                    self.step,
                    step_seconds,
                    np.mean(self.losses),
                    batchsize
                )
            )
            self.losses.clear()
        self.step += 1
```

## 完成训练

具体可查看相应目录下的log_800.out日志文件

