# 求导

反向传播梯度优化是深度学习最核心的训练思路（不清楚的同学可以去找找相关资料先入门深度学习）。

## 函数式自动微分

函数式自动微分其实就是把反向梯度传播的过程封装到固定的函数里，能够非常方便地调用得到。

```python
x = ops.ones(5, mindspore.float32)  # input tensor
y = ops.zeros(3, mindspore.float32)  # expected output
w = Parameter(Tensor(np.random.randn(5, 3), mindspore.float32), name='w') # weight
b = Parameter(Tensor(np.random.randn(3,), mindspore.float32), name='b') # bias


def function(x, y, w, b):
    z = ops.matmul(x, w) + b
    # 二值交叉熵作为损失函数
    loss = ops.binary_cross_entropy_with_logits(z, y, ops.ones_like(z), ops.ones_like(z))
    return loss

loss = function(x, y, w, b)
print(loss)

"""
Tensor(shape=[], dtype=Float32, value= 0.914285)
"""
```

上述代码完成了一次前向传播（输入数据x，经过网络[w,b]，处理结果与y计算损失函数）。其反向传播可通过调用grad函数直接实现

```python
# 参数解释：arg1: 求导的函数； arg2：求导输入的位置索引。 示例中需要对w和b求导
# grad会返回一个对象
grad_fn = mindspore.grad(function, (2, 3)) 
```
通过给这个对象喂数据，就能计算出刚刚设置的位置的梯度
```python
grads = grad_fn(x, y, w, b)
print(grads)

"""
(Tensor(shape=[5, 3], dtype=Float32, value=
 [[ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01]]),
 Tensor(shape=[3], dtype=Float32, value= [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01]))
"""
```

## 神经网络的梯度计算

先声明一个网络

```python
# Define model
class Network(nn.Cell):
    def __init__(self):
        super().__init__()
        self.w = w
        self.b = b

    def construct(self, x):
        z = ops.matmul(x, self.w) + self.b
        return z
```

实例化模型和损失函数

```python
# Instantiate model
model = Network()
# Instantiate loss function
loss_fn = nn.BCEWithLogitsLoss()
```

因为需要调用grad函数去求导，所以需要把前向计算封装到一个函数里去

```python
def forward_fn(x, y):
    z = model(x)
    loss = loss_fn(z, y)
    return loss
```

因为参数是放到Cell里的，所以不需要指定grad_position了，设置为None，而是使用weights

```python
grad_fn = mindspore.value_and_grad(forward_fn, None, weights=model.trainable_params())
loss, grads = grad_fn(x, y)
print(grads)


"""
(Tensor(shape=[5, 3], dtype=Float32, value=
 [[ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01],
  [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01]]),
 Tensor(shape=[3], dtype=Float32, value= [ 6.56869709e-02,  5.37334494e-02,  3.01467031e-01]))
"""
```